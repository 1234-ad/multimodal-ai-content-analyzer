{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Multi-Modal AI Research Notebook\n",
    "\n",
    "This notebook demonstrates cutting-edge research and experimentation with multi-modal AI models, including:\n",
    "- Custom model architectures\n",
    "- Advanced training techniques\n",
    "- Multi-modal fusion strategies\n",
    "- Performance optimization\n",
    "- Research experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    BlipProcessor, BlipForConditionalGeneration\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Multi-Modal Architecture Design\n",
    "\n",
    "Let's design a state-of-the-art multi-modal architecture that combines vision and language understanding with advanced fusion techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Cross-modal attention mechanism for vision-language fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_dim, text_dim, hidden_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Vision projections\n",
    "        self.vision_query = nn.Linear(vision_dim, hidden_dim)\n",
    "        self.vision_key = nn.Linear(vision_dim, hidden_dim)\n",
    "        self.vision_value = nn.Linear(vision_dim, hidden_dim)\n",
    "        \n",
    "        # Text projections\n",
    "        self.text_query = nn.Linear(text_dim, hidden_dim)\n",
    "        self.text_key = nn.Linear(text_dim, hidden_dim)\n",
    "        self.text_value = nn.Linear(text_dim, hidden_dim)\n",
    "        \n",
    "        # Output projections\n",
    "        self.vision_out = nn.Linear(hidden_dim, vision_dim)\n",
    "        self.text_out = nn.Linear(hidden_dim, text_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm_v = nn.LayerNorm(vision_dim)\n",
    "        self.layer_norm_t = nn.LayerNorm(text_dim)\n",
    "    \n",
    "    def forward(self, vision_features, text_features):\n",
    "        batch_size = vision_features.size(0)\n",
    "        \n",
    "        # Vision to Text attention\n",
    "        v_q = self.vision_query(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        t_k = self.text_key(text_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        t_v = self.text_value(text_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        v2t_scores = torch.matmul(v_q, t_k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        v2t_attn = torch.softmax(v2t_scores, dim=-1)\n",
    "        v2t_out = torch.matmul(v2t_attn, t_v)\n",
    "        \n",
    "        # Text to Vision attention\n",
    "        t_q = self.text_query(text_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v_k = self.vision_key(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v_v = self.vision_value(vision_features).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        t2v_scores = torch.matmul(t_q, v_k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        t2v_attn = torch.softmax(t2v_scores, dim=-1)\n",
    "        t2v_out = torch.matmul(t2v_attn, v_v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        v2t_out = v2t_out.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        t2v_out = t2v_out.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        \n",
    "        # Apply output projections and residual connections\n",
    "        enhanced_vision = self.layer_norm_v(vision_features + self.dropout(self.vision_out(t2v_out.mean(1))))\n",
    "        enhanced_text = self.layer_norm_t(text_features + self.dropout(self.text_out(v2t_out.mean(1))))\n",
    "        \n",
    "        return enhanced_vision, enhanced_text\n",
    "\n",
    "class AdvancedMultiModalModel(nn.Module):\n",
    "    \"\"\"Advanced multi-modal model with cross-attention and hierarchical fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_model_name, text_model_name, num_classes, fusion_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained encoders\n",
    "        self.vision_encoder = AutoModel.from_pretrained(vision_model_name)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        \n",
    "        # Get dimensions\n",
    "        self.vision_dim = self.vision_encoder.config.hidden_size\n",
    "        self.text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Cross-modal attention layers\n",
    "        self.cross_attention_layers = nn.ModuleList([\n",
    "            CrossModalAttention(self.vision_dim, self.text_dim, 512)\n",
    "            for _ in range(fusion_layers)\n",
    "        ])\n",
    "        \n",
    "        # Hierarchical fusion\n",
    "        self.fusion_dim = 256\n",
    "        self.vision_proj = nn.Linear(self.vision_dim, self.fusion_dim)\n",
    "        self.text_proj = nn.Linear(self.text_dim, self.fusion_dim)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        self.early_fusion = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "        self.mid_fusion = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "        self.late_fusion = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "        \n",
    "        # Classification head with attention pooling\n",
    "        self.attention_pool = nn.MultiheadAttention(self.fusion_dim, num_heads=8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.fusion_dim, self.fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.fusion_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary losses for better training\n",
    "        self.vision_aux_classifier = nn.Linear(self.vision_dim, num_classes)\n",
    "        self.text_aux_classifier = nn.Linear(self.text_dim, num_classes)\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids, attention_mask, return_aux=False):\n",
    "        # Encode modalities\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        vision_features = vision_outputs.pooler_output\n",
    "        text_features = text_outputs.pooler_output\n",
    "        \n",
    "        # Store original features for auxiliary losses\n",
    "        orig_vision_features = vision_features.clone()\n",
    "        orig_text_features = text_features.clone()\n",
    "        \n",
    "        # Apply cross-modal attention layers\n",
    "        for cross_attn in self.cross_attention_layers:\n",
    "            vision_features, text_features = cross_attn(vision_features, text_features)\n",
    "        \n",
    "        # Project to fusion dimension\n",
    "        vision_proj = self.vision_proj(vision_features)\n",
    "        text_proj = self.text_proj(text_features)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        early_fused = torch.tanh(self.early_fusion(torch.cat([vision_proj, text_proj], dim=-1)))\n",
    "        mid_fused = torch.tanh(self.mid_fusion(torch.cat([vision_proj * text_proj, vision_proj + text_proj], dim=-1)))\n",
    "        late_fused = torch.tanh(self.late_fusion(torch.cat([early_fused, mid_fused], dim=-1)))\n",
    "        \n",
    "        # Attention pooling\n",
    "        fusion_stack = torch.stack([early_fused, mid_fused, late_fused], dim=1)\n",
    "        attended_features, _ = self.attention_pool(fusion_stack, fusion_stack, fusion_stack)\n",
    "        final_features = attended_features.mean(dim=1)\n",
    "        \n",
    "        # Main classification\n",
    "        logits = self.classifier(final_features)\n",
    "        \n",
    "        if return_aux:\n",
    "            # Auxiliary classifications for regularization\n",
    "            vision_aux_logits = self.vision_aux_classifier(orig_vision_features)\n",
    "            text_aux_logits = self.text_aux_classifier(orig_text_features)\n",
    "            return logits, vision_aux_logits, text_aux_logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "model = AdvancedMultiModalModel(\n",
    "    vision_model_name=\"google/vit-base-patch16-224\",\n",
    "    text_model_name=\"bert-base-uncased\",\n",
    "    num_classes=10,\n",
    "    fusion_layers=3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Training Techniques\n",
    "\n",
    "Implement cutting-edge training techniques including:\n",
    "- Curriculum learning\n",
    "- Contrastive learning\n",
    "- Knowledge distillation\n",
    "- Advanced optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive loss for multi-modal representation learning\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_sim = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def forward(self, vision_features, text_features):\n",
    "        batch_size = vision_features.size(0)\n",
    "        \n",
    "        # Normalize features\n",
    "        vision_features = nn.functional.normalize(vision_features, dim=-1)\n",
    "        text_features = nn.functional.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(vision_features, text_features.T) / self.temperature\n",
    "        \n",
    "        # Create labels (diagonal should be positive pairs)\n",
    "        labels = torch.arange(batch_size).to(vision_features.device)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        loss_v2t = nn.functional.cross_entropy(similarity_matrix, labels)\n",
    "        loss_t2v = nn.functional.cross_entropy(similarity_matrix.T, labels)\n",
    "        \n",
    "        return (loss_v2t + loss_t2v) / 2\n",
    "\n",
    "class CurriculumLearningScheduler:\n",
    "    \"\"\"Curriculum learning scheduler for progressive difficulty\"\"\"\n",
    "    \n",
    "    def __init__(self, total_epochs, difficulty_levels=5):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.difficulty_levels = difficulty_levels\n",
    "        self.current_level = 1\n",
    "    \n",
    "    def get_difficulty_level(self, epoch):\n",
    "        \"\"\"Get current difficulty level based on epoch\"\"\"\n",
    "        progress = epoch / self.total_epochs\n",
    "        level = min(int(progress * self.difficulty_levels) + 1, self.difficulty_levels)\n",
    "        return level\n",
    "    \n",
    "    def should_include_sample(self, sample_difficulty, current_level):\n",
    "        \"\"\"Determine if sample should be included based on difficulty\"\"\"\n",
    "        return sample_difficulty <= current_level\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    \"\"\"Advanced trainer with multiple training techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=config['warmup_epochs'],\n",
    "            T_mult=2,\n",
    "            eta_min=config['learning_rate'] * 0.01\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.classification_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.contrastive_loss = ContrastiveLoss(temperature=0.07)\n",
    "        \n",
    "        # Curriculum learning\n",
    "        self.curriculum_scheduler = CurriculumLearningScheduler(config['epochs'])\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if config['mixed_precision'] else None\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch with advanced techniques\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Get current curriculum difficulty level\n",
    "        difficulty_level = self.curriculum_scheduler.get_difficulty_level(epoch)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            # Curriculum learning sample filtering\n",
    "            if hasattr(batch, 'difficulty'):\n",
    "                valid_samples = [\n",
    "                    i for i, diff in enumerate(batch['difficulty'])\n",
    "                    if self.curriculum_scheduler.should_include_sample(diff, difficulty_level)\n",
    "                ]\n",
    "                if not valid_samples:\n",
    "                    continue\n",
    "                \n",
    "                # Filter batch based on curriculum\n",
    "                batch = {k: v[valid_samples] if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "            \n",
    "            # Move to device\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass with auxiliary outputs\n",
    "                    logits, vision_aux, text_aux = self.model(\n",
    "                        pixel_values, input_ids, attention_mask, return_aux=True\n",
    "                    )\n",
    "                    \n",
    "                    # Multi-task loss\n",
    "                    main_loss = self.classification_loss(logits, labels)\n",
    "                    aux_loss_v = self.classification_loss(vision_aux, labels)\n",
    "                    aux_loss_t = self.classification_loss(text_aux, labels)\n",
    "                    \n",
    "                    # Contrastive loss for representation learning\n",
    "                    vision_features = self.model.vision_encoder(pixel_values=pixel_values).pooler_output\n",
    "                    text_features = self.model.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "                    contrastive_loss = self.contrastive_loss(vision_features, text_features)\n",
    "                    \n",
    "                    # Combined loss\n",
    "                    total_batch_loss = (\n",
    "                        main_loss + \n",
    "                        0.3 * aux_loss_v + \n",
    "                        0.3 * aux_loss_t + \n",
    "                        0.2 * contrastive_loss\n",
    "                    )\n",
    "                \n",
    "                self.scaler.scale(total_batch_loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                # Standard training without mixed precision\n",
    "                logits, vision_aux, text_aux = self.model(\n",
    "                    pixel_values, input_ids, attention_mask, return_aux=True\n",
    "                )\n",
    "                \n",
    "                main_loss = self.classification_loss(logits, labels)\n",
    "                aux_loss_v = self.classification_loss(vision_aux, labels)\n",
    "                aux_loss_t = self.classification_loss(text_aux, labels)\n",
    "                \n",
    "                vision_features = self.model.vision_encoder(pixel_values=pixel_values).pooler_output\n",
    "                text_features = self.model.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "                contrastive_loss = self.contrastive_loss(vision_features, text_features)\n",
    "                \n",
    "                total_batch_loss = (\n",
    "                    main_loss + \n",
    "                    0.3 * aux_loss_v + \n",
    "                    0.3 * aux_loss_t + \n",
    "                    0.2 * contrastive_loss\n",
    "                )\n",
    "                \n",
    "                total_batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            total_loss += total_batch_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {total_batch_loss.item():.4f}, '\n",
    "                      f'Difficulty Level: {difficulty_level}, LR: {self.scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits = self.model(pixel_values, input_ids, attention_mask)\n",
    "                loss = self.classification_loss(logits, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        self.val_losses.append(avg_loss)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        best_accuracy = 0\n",
    "        patience = 0\n",
    "        max_patience = self.config.get('patience', 10)\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.config['epochs']}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Training\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                patience = 0\n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'best_accuracy': best_accuracy,\n",
    "                }, 'best_model.pt')\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        return best_accuracy\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_epochs': 5,\n",
    "    'mixed_precision': True,\n",
    "    'patience': 10\n",
    "}\n",
    "\n",
    "print(\"Advanced training configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Modal Data Analysis and Visualization\n",
    "\n",
    "Analyze multi-modal data patterns and create advanced visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalAnalyzer:\n",
    "    \"\"\"Advanced analyzer for multi-modal data patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def extract_features(self, dataloader, max_samples=1000):\n",
    "        \"\"\"Extract features from multi-modal data\"\"\"\n",
    "        vision_features = []\n",
    "        text_features = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                if i * dataloader.batch_size >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Extract features from encoders\n",
    "                vision_output = self.model.vision_encoder(pixel_values=pixel_values)\n",
    "                text_output = self.model.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                vision_features.append(vision_output.pooler_output.cpu().numpy())\n",
    "                text_features.append(text_output.pooler_output.cpu().numpy())\n",
    "                labels.append(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        vision_features = np.vstack(vision_features)\n",
    "        text_features = np.vstack(text_features)\n",
    "        labels = np.hstack(labels)\n",
    "        \n",
    "        return vision_features, text_features, labels\n",
    "    \n",
    "    def analyze_feature_correlation(self, vision_features, text_features):\n",
    "        \"\"\"Analyze correlation between vision and text features\"\"\"\n",
    "        # Compute correlation matrix\n",
    "        combined_features = np.hstack([vision_features, text_features])\n",
    "        correlation_matrix = np.corrcoef(combined_features.T)\n",
    "        \n",
    "        # Visualize correlation\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix[:50, :50], cmap='coolwarm', center=0, \n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title('Feature Correlation Matrix (First 50 dimensions)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return correlation_matrix\n",
    "    \n",
    "    def visualize_feature_space(self, vision_features, text_features, labels, method='tsne'):\n",
    "        \"\"\"Visualize feature space using dimensionality reduction\"\"\"\n",
    "        # Combine features\n",
    "        combined_features = np.hstack([vision_features, text_features])\n",
    "        \n",
    "        # Apply dimensionality reduction\n",
    "        if method == 'tsne':\n",
    "            reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        else:\n",
    "            from sklearn.decomposition import PCA\n",
    "            reducer = PCA(n_components=2, random_state=42)\n",
    "        \n",
    "        reduced_features = reducer.fit_transform(combined_features)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Combined features\n",
    "        plt.subplot(1, 3, 1)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                            c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'Combined Features ({method.upper()})')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        \n",
    "        # Vision features only\n",
    "        vision_reduced = reducer.fit_transform(vision_features)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        scatter = plt.scatter(vision_reduced[:, 0], vision_reduced[:, 1], \n",
    "                            c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'Vision Features ({method.upper()})')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        \n",
    "        # Text features only\n",
    "        text_reduced = reducer.fit_transform(text_features)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        scatter = plt.scatter(text_reduced[:, 0], text_reduced[:, 1], \n",
    "                            c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'Text Features ({method.upper()})')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return reduced_features\n",
    "    \n",
    "    def analyze_modality_importance(self, dataloader, num_samples=100):\n",
    "        \"\"\"Analyze the importance of each modality for predictions\"\"\"\n",
    "        vision_only_correct = 0\n",
    "        text_only_correct = 0\n",
    "        combined_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                if i >= num_samples // dataloader.batch_size:\n",
    "                    break\n",
    "                \n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Combined prediction\n",
    "                combined_logits = self.model(pixel_values, input_ids, attention_mask)\n",
    "                combined_pred = torch.argmax(combined_logits, dim=1)\n",
    "                \n",
    "                # Vision-only prediction (zero out text)\n",
    "                zero_input_ids = torch.zeros_like(input_ids)\n",
    "                zero_attention_mask = torch.zeros_like(attention_mask)\n",
    "                vision_logits = self.model(pixel_values, zero_input_ids, zero_attention_mask)\n",
    "                vision_pred = torch.argmax(vision_logits, dim=1)\n",
    "                \n",
    "                # Text-only prediction (zero out vision)\n",
    "                zero_pixel_values = torch.zeros_like(pixel_values)\n",
    "                text_logits = self.model(zero_pixel_values, input_ids, attention_mask)\n",
    "                text_pred = torch.argmax(text_logits, dim=1)\n",
    "                \n",
    "                # Count correct predictions\n",
    "                vision_only_correct += (vision_pred == labels).sum().item()\n",
    "                text_only_correct += (text_pred == labels).sum().item()\n",
    "                combined_correct += (combined_pred == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        vision_accuracy = vision_only_correct / total\n",
    "        text_accuracy = text_only_correct / total\n",
    "        combined_accuracy = combined_correct / total\n",
    "        \n",
    "        # Visualize results\n",
    "        modalities = ['Vision Only', 'Text Only', 'Combined']\n",
    "        accuracies = [vision_accuracy, text_accuracy, combined_accuracy]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(modalities, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Modality Importance Analysis')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{acc:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'vision_accuracy': vision_accuracy,\n",
    "            'text_accuracy': text_accuracy,\n",
    "            'combined_accuracy': combined_accuracy,\n",
    "            'synergy_effect': combined_accuracy - max(vision_accuracy, text_accuracy)\n",
    "        }\n",
    "    \n",
    "    def cluster_analysis(self, vision_features, text_features, labels, n_clusters=5):\n",
    "        \"\"\"Perform clustering analysis on multi-modal features\"\"\"\n",
    "        # Combine features\n",
    "        combined_features = np.hstack([vision_features, text_features])\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(combined_features)\n",
    "        \n",
    "        # Visualize clusters\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        reduced_features = tsne.fit_transform(combined_features)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # True labels\n",
    "        plt.subplot(1, 3, 1)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                            c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title('True Labels')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        \n",
    "        # Cluster labels\n",
    "        plt.subplot(1, 3, 2)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                            c=cluster_labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title('K-Means Clusters')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        \n",
    "        # Cluster centers\n",
    "        plt.subplot(1, 3, 3)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                            c=cluster_labels, cmap='tab10', alpha=0.7)\n",
    "        \n",
    "        # Transform cluster centers to t-SNE space (approximate)\n",
    "        centers_2d = tsne.fit_transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centers_2d[:, 0], centers_2d[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title('Clusters with Centers')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze cluster purity\n",
    "        from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "        \n",
    "        ari_score = adjusted_rand_score(labels, cluster_labels)\n",
    "        nmi_score = normalized_mutual_info_score(labels, cluster_labels)\n",
    "        \n",
    "        print(f\"Clustering Analysis Results:\")\n",
    "        print(f\"Adjusted Rand Index: {ari_score:.3f}\")\n",
    "        print(f\"Normalized Mutual Information: {nmi_score:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'cluster_centers': kmeans.cluster_centers_,\n",
    "            'ari_score': ari_score,\n",
    "            'nmi_score': nmi_score\n",
    "        }\n",
    "\n",
    "print(\"Multi-modal analyzer ready for advanced data analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Interpretability and Explainability\n",
    "\n",
    "Implement advanced techniques for understanding model decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalExplainer:\n",
    "    \"\"\"Advanced explainability for multi-modal models\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def generate_attention_maps(self, pixel_values, input_ids, attention_mask):\n",
    "        \"\"\"Generate attention maps for vision and text modalities\"\"\"\n",
    "        # Enable gradient computation\n",
    "        pixel_values.requires_grad_()\n",
    "        input_ids.requires_grad_()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_class = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        logits[0, pred_class].backward(retain_graph=True)\n",
    "        \n",
    "        # Vision attention (gradient-based)\n",
    "        vision_gradients = pixel_values.grad.abs().mean(dim=1)  # Average over channels\n",
    "        \n",
    "        # Text attention (gradient-based)\n",
    "        text_gradients = input_ids.grad.abs() if input_ids.grad is not None else torch.zeros_like(input_ids)\n",
    "        \n",
    "        return vision_gradients, text_gradients\n",
    "    \n",
    "    def integrated_gradients(self, pixel_values, input_ids, attention_mask, steps=50):\n",
    "        \"\"\"Compute integrated gradients for better attribution\"\"\"\n",
    "        # Baseline (zeros)\n",
    "        baseline_pixels = torch.zeros_like(pixel_values)\n",
    "        baseline_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # Generate path\n",
    "        alphas = torch.linspace(0, 1, steps).to(pixel_values.device)\n",
    "        \n",
    "        vision_gradients = torch.zeros_like(pixel_values)\n",
    "        text_gradients = torch.zeros_like(input_ids, dtype=torch.float)\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            # Interpolated inputs\n",
    "            interp_pixels = baseline_pixels + alpha * (pixel_values - baseline_pixels)\n",
    "            interp_ids = baseline_ids + alpha * (input_ids - baseline_ids)\n",
    "            \n",
    "            interp_pixels.requires_grad_()\n",
    "            interp_ids.requires_grad_()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.model(interp_pixels, interp_ids.long(), attention_mask)\n",
    "            pred_class = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Backward pass\n",
    "            logits[0, pred_class].backward(retain_graph=True)\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            if interp_pixels.grad is not None:\n",
    "                vision_gradients += interp_pixels.grad\n",
    "            if interp_ids.grad is not None:\n",
    "                text_gradients += interp_ids.grad.float()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.model.zero_grad()\n",
    "        \n",
    "        # Average gradients and multiply by input difference\n",
    "        vision_gradients = vision_gradients / steps * (pixel_values - baseline_pixels)\n",
    "        text_gradients = text_gradients / steps * (input_ids.float() - baseline_ids.float())\n",
    "        \n",
    "        return vision_gradients, text_gradients\n",
    "    \n",
    "    def visualize_attention(self, image, text, tokenizer, method='integrated_gradients'):\n",
    "        \"\"\"Visualize attention for a single example\"\"\"\n",
    "        # Preprocess inputs\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        pixel_values = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        # Generate attributions\n",
    "        if method == 'integrated_gradients':\n",
    "            vision_attr, text_attr = self.integrated_gradients(pixel_values, input_ids, attention_mask)\n",
    "        else:\n",
    "            vision_attr, text_attr = self.generate_attention_maps(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Visualize results\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(image)\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Vision attention heatmap\n",
    "        vision_heatmap = vision_attr[0].cpu().detach().numpy().mean(axis=0)\n",
    "        im = axes[0, 1].imshow(vision_heatmap, cmap='hot', interpolation='bilinear')\n",
    "        axes[0, 1].set_title('Vision Attention Heatmap')\n",
    "        axes[0, 1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[0, 1])\n",
    "        \n",
    "        # Text attention\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        text_scores = text_attr[0].cpu().detach().numpy()\n",
    "        \n",
    "        # Filter out special tokens and padding\n",
    "        valid_tokens = [(token, score) for token, score in zip(tokens, text_scores) \n",
    "                       if token not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "        \n",
    "        if valid_tokens:\n",
    "            tokens_filtered, scores_filtered = zip(*valid_tokens)\n",
    "            \n",
    "            # Normalize scores for visualization\n",
    "            scores_normalized = np.array(scores_filtered)\n",
    "            scores_normalized = (scores_normalized - scores_normalized.min()) / (scores_normalized.max() - scores_normalized.min() + 1e-8)\n",
    "            \n",
    "            # Create text attention visualization\n",
    "            axes[1, 0].barh(range(len(tokens_filtered)), scores_normalized)\n",
    "            axes[1, 0].set_yticks(range(len(tokens_filtered)))\n",
    "            axes[1, 0].set_yticklabels(tokens_filtered, fontsize=8)\n",
    "            axes[1, 0].set_xlabel('Attention Score')\n",
    "            axes[1, 0].set_title('Text Token Attention')\n",
    "        \n",
    "        # Combined visualization (overlay)\n",
    "        axes[1, 1].imshow(image, alpha=0.7)\n",
    "        axes[1, 1].imshow(vision_heatmap, cmap='hot', alpha=0.3, interpolation='bilinear')\n",
    "        axes[1, 1].set_title('Image + Attention Overlay')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return vision_attr, text_attr\n",
    "    \n",
    "    def analyze_cross_modal_interactions(self, dataloader, num_samples=50):\n",
    "        \"\"\"Analyze how vision and text modalities interact\"\"\"\n",
    "        interaction_scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                if i >= num_samples // dataloader.batch_size:\n",
    "                    break\n",
    "                \n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Get features from each modality\n",
    "                vision_features = self.model.vision_encoder(pixel_values=pixel_values).pooler_output\n",
    "                text_features = self.model.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "                \n",
    "                # Compute interaction score (cosine similarity)\n",
    "                interaction = torch.cosine_similarity(vision_features, text_features, dim=1)\n",
    "                interaction_scores.extend(interaction.cpu().numpy())\n",
    "        \n",
    "        # Visualize interaction distribution\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(interaction_scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Cross-Modal Similarity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Vision-Text Interactions')\n",
    "        plt.axvline(np.mean(interaction_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(interaction_scores):.3f}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(interaction_scores)\n",
    "        plt.ylabel('Cross-Modal Similarity')\n",
    "        plt.title('Interaction Score Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'mean_interaction': np.mean(interaction_scores),\n",
    "            'std_interaction': np.std(interaction_scores),\n",
    "            'min_interaction': np.min(interaction_scores),\n",
    "            'max_interaction': np.max(interaction_scores)\n",
    "        }\n",
    "    \n",
    "    def feature_importance_analysis(self, dataloader, num_samples=100):\n",
    "        \"\"\"Analyze feature importance across modalities\"\"\"\n",
    "        vision_importance = []\n",
    "        text_importance = []\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_samples // dataloader.batch_size:\n",
    "                break\n",
    "            \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Compute feature importance using permutation\n",
    "            with torch.no_grad():\n",
    "                # Original prediction\n",
    "                original_logits = self.model(pixel_values, input_ids, attention_mask)\n",
    "                original_confidence = torch.softmax(original_logits, dim=1).max(dim=1)[0]\n",
    "                \n",
    "                # Vision importance (permute vision features)\n",
    "                shuffled_pixels = pixel_values[torch.randperm(pixel_values.size(0))]\n",
    "                vision_logits = self.model(shuffled_pixels, input_ids, attention_mask)\n",
    "                vision_confidence = torch.softmax(vision_logits, dim=1).max(dim=1)[0]\n",
    "                vision_drop = (original_confidence - vision_confidence).mean().item()\n",
    "                \n",
    "                # Text importance (permute text features)\n",
    "                shuffled_ids = input_ids[torch.randperm(input_ids.size(0))]\n",
    "                shuffled_mask = attention_mask[torch.randperm(attention_mask.size(0))]\n",
    "                text_logits = self.model(pixel_values, shuffled_ids, shuffled_mask)\n",
    "                text_confidence = torch.softmax(text_logits, dim=1).max(dim=1)[0]\n",
    "                text_drop = (original_confidence - text_confidence).mean().item()\n",
    "                \n",
    "                vision_importance.append(vision_drop)\n",
    "                text_importance.append(text_drop)\n",
    "        \n",
    "        # Visualize importance\n",
    "        modalities = ['Vision', 'Text']\n",
    "        importance_scores = [np.mean(vision_importance), np.mean(text_importance)]\n",
    "        importance_std = [np.std(vision_importance), np.std(text_importance)]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(modalities, importance_scores, yerr=importance_std, \n",
    "                      capsize=5, color=['skyblue', 'lightcoral'])\n",
    "        plt.ylabel('Importance Score (Confidence Drop)')\n",
    "        plt.title('Feature Importance Analysis')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, importance_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'vision_importance': np.mean(vision_importance),\n",
    "            'text_importance': np.mean(text_importance),\n",
    "            'vision_std': np.std(vision_importance),\n",
    "            'text_std': np.std(text_importance)\n",
    "        }\n",
    "\n",
    "print(\"Multi-modal explainer ready for interpretability analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization and Benchmarking\n",
    "\n",
    "Advanced optimization techniques and comprehensive benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"Advanced performance optimization for multi-modal models\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.original_model = None\n",
    "    \n",
    "    def apply_torch_compile(self):\n",
    "        \"\"\"Apply PyTorch 2.0 compilation for speed optimization\"\"\"\n",
    "        if hasattr(torch, 'compile'):\n",
    "            print(\"Applying torch.compile optimization...\")\n",
    "            self.original_model = self.model\n",
    "            self.model = torch.compile(self.model, mode='max-autotune')\n",
    "            print(\"✅ Torch compilation applied\")\n",
    "        else:\n",
    "            print(\"⚠️ torch.compile not available in this PyTorch version\")\n",
    "    \n",
    "    def apply_quantization(self, quantization_type='dynamic'):\n",
    "        \"\"\"Apply model quantization for memory and speed optimization\"\"\"\n",
    "        print(f\"Applying {quantization_type} quantization...\")\n",
    "        \n",
    "        if quantization_type == 'dynamic':\n",
    "            self.model = torch.quantization.quantize_dynamic(\n",
    "                self.model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "            )\n",
    "        elif quantization_type == 'static':\n",
    "            # Static quantization requires calibration data\n",
    "            self.model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "            torch.quantization.prepare(self.model, inplace=True)\n",
    "            # Calibration would happen here with representative data\n",
    "            torch.quantization.convert(self.model, inplace=True)\n",
    "        \n",
    "        print(f\"✅ {quantization_type.capitalize()} quantization applied\")\n",
    "    \n",
    "    def apply_pruning(self, sparsity=0.3):\n",
    "        \"\"\"Apply structured pruning to reduce model size\"\"\"\n",
    "        print(f\"Applying pruning with {sparsity*100}% sparsity...\")\n",
    "        \n",
    "        import torch.nn.utils.prune as prune\n",
    "        \n",
    "        # Apply pruning to linear layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=sparsity)\n",
    "                prune.remove(module, 'weight')\n",
    "        \n",
    "        print(f\"✅ Pruning applied with {sparsity*100}% sparsity\")\n",
    "    \n",
    "    def optimize_memory(self):\n",
    "        \"\"\"Apply memory optimization techniques\"\"\"\n",
    "        print(\"Applying memory optimizations...\")\n",
    "        \n",
    "        # Enable gradient checkpointing\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Enable attention slicing for vision models\n",
    "        if hasattr(self.model, 'enable_attention_slicing'):\n",
    "            self.model.enable_attention_slicing()\n",
    "        \n",
    "        # Enable memory efficient attention\n",
    "        if hasattr(self.model, 'enable_xformers_memory_efficient_attention'):\n",
    "            try:\n",
    "                self.model.enable_xformers_memory_efficient_attention()\n",
    "                print(\"✅ XFormers memory efficient attention enabled\")\n",
    "            except:\n",
    "                print(\"⚠️ XFormers not available\")\n",
    "        \n",
    "        print(\"✅ Memory optimizations applied\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Comprehensive performance benchmarking suite\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    @contextmanager\n",
    "    def measure_time(self, operation_name):\n",
    "        \"\"\"Context manager for measuring execution time\"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = self.get_memory_usage()\n",
    "        \n",
    "        yield\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_memory = self.get_memory_usage()\n",
    "        \n",
    "        self.results[operation_name] = {\n",
    "            'execution_time': end_time - start_time,\n",
    "            'memory_before': start_memory,\n",
    "            'memory_after': end_memory,\n",
    "            'memory_delta': end_memory['gpu_used'] - start_memory['gpu_used'] if torch.cuda.is_available() else 0\n",
    "        }\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        memory_info = {\n",
    "            'cpu_percent': psutil.virtual_memory().percent,\n",
    "            'cpu_used_gb': psutil.virtual_memory().used / (1024**3)\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu = GPUtil.getGPUs()[0] if GPUtil.getGPUs() else None\n",
    "            if gpu:\n",
    "                memory_info.update({\n",
    "                    'gpu_used': gpu.memoryUsed,\n",
    "                    'gpu_total': gpu.memoryTotal,\n",
    "                    'gpu_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100\n",
    "                })\n",
    "            else:\n",
    "                memory_info.update({\n",
    "                    'gpu_used': torch.cuda.memory_allocated() / (1024**3),\n",
    "                    'gpu_total': torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
    "                    'gpu_percent': (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100\n",
    "                })\n",
    "        \n",
    "        return memory_info\n",
    "    \n",
    "    def benchmark_inference(self, model, dataloader, num_batches=10):\n",
    "        \"\"\"Benchmark inference performance\"\"\"\n",
    "        model.eval()\n",
    "        inference_times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                if i >= num_batches:\n",
    "                    break\n",
    "                \n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                with self.measure_time(f'inference_batch_{i}'):\n",
    "                    logits = model(pixel_values, input_ids, attention_mask)\n",
    "                \n",
    "                inference_times.append(self.results[f'inference_batch_{i}']['execution_time'])\n",
    "        \n",
    "        return {\n",
    "            'mean_inference_time': np.mean(inference_times),\n",
    "            'std_inference_time': np.std(inference_times),\n",
    "            'min_inference_time': np.min(inference_times),\n",
    "            'max_inference_time': np.max(inference_times),\n",
    "            'throughput_samples_per_second': dataloader.batch_size / np.mean(inference_times)\n",
    "        }\n",
    "    \n",
    "    def benchmark_training_step(self, model, dataloader, optimizer, criterion, num_steps=5):\n",
    "        \"\"\"Benchmark training step performance\"\"\"\n",
    "        model.train()\n",
    "        training_times = []\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_steps:\n",
    "                break\n",
    "            \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with self.measure_time(f'training_step_{i}'):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(pixel_values, input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            training_times.append(self.results[f'training_step_{i}']['execution_time'])\n",
    "        \n",
    "        return {\n",
    "            'mean_training_time': np.mean(training_times),\n",
    "            'std_training_time': np.std(training_times),\n",
    "            'min_training_time': np.min(training_times),\n",
    "            'max_training_time': np.max(training_times)\n",
    "        }\n",
    "    \n",
    "    def compare_optimizations(self, original_model, optimized_models, dataloader):\n",
    "        \"\"\"Compare performance of different optimizations\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Benchmark original model\n",
    "        print(\"Benchmarking original model...\")\n",
    "        results['original'] = self.benchmark_inference(original_model, dataloader)\n",
    "        \n",
    "        # Benchmark optimized models\n",
    "        for name, model in optimized_models.items():\n",
    "            print(f\"Benchmarking {name} model...\")\n",
    "            results[name] = self.benchmark_inference(model, dataloader)\n",
    "        \n",
    "        # Visualize results\n",
    "        self.visualize_benchmark_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_benchmark_results(self, results):\n",
    "        \"\"\"Visualize benchmark results\"\"\"\n",
    "        models = list(results.keys())\n",
    "        inference_times = [results[model]['mean_inference_time'] for model in models]\n",
    "        throughputs = [results[model]['throughput_samples_per_second'] for model in models]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Inference time comparison\n",
    "        bars1 = ax1.bar(models, inference_times, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'][:len(models)])\n",
    "        ax1.set_ylabel('Inference Time (seconds)')\n",
    "        ax1.set_title('Inference Time Comparison')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, time_val in zip(bars1, inference_times):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                    f'{time_val:.3f}s', ha='center', va='bottom')\n",
    "        \n",
    "        # Throughput comparison\n",
    "        bars2 = ax2.bar(models, throughputs, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'][:len(models)])\n",
    "        ax2.set_ylabel('Throughput (samples/second)')\n",
    "        ax2.set_title('Throughput Comparison')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, throughput in zip(bars2, throughputs):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    f'{throughput:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print speedup information\n",
    "        if 'original' in results:\n",
    "            original_time = results['original']['mean_inference_time']\n",
    "            print(\"\\nSpeedup Analysis:\")\n",
    "            print(\"-\" * 40)\n",
    "            for model in models:\n",
    "                if model != 'original':\n",
    "                    speedup = original_time / results[model]['mean_inference_time']\n",
    "                    print(f\"{model}: {speedup:.2f}x speedup\")\n",
    "    \n",
    "    def profile_model_components(self, model, sample_input):\n",
    "        \"\"\"Profile individual model components\"\"\"\n",
    "        pixel_values, input_ids, attention_mask = sample_input\n",
    "        \n",
    "        # Profile vision encoder\n",
    "        with self.measure_time('vision_encoder'):\n",
    "            vision_output = model.vision_encoder(pixel_values=pixel_values)\n",
    "        \n",
    "        # Profile text encoder\n",
    "        with self.measure_time('text_encoder'):\n",
    "            text_output = model.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Profile fusion layers\n",
    "        vision_features = vision_output.pooler_output\n",
    "        text_features = text_output.pooler_output\n",
    "        \n",
    "        with self.measure_time('cross_attention'):\n",
    "            for cross_attn in model.cross_attention_layers:\n",
    "                vision_features, text_features = cross_attn(vision_features, text_features)\n",
    "        \n",
    "        # Profile classifier\n",
    "        vision_proj = model.vision_proj(vision_features)\n",
    "        text_proj = model.text_proj(text_features)\n",
    "        \n",
    "        with self.measure_time('classifier'):\n",
    "            early_fused = torch.tanh(model.early_fusion(torch.cat([vision_proj, text_proj], dim=-1)))\n",
    "            mid_fused = torch.tanh(model.mid_fusion(torch.cat([vision_proj * text_proj, vision_proj + text_proj], dim=-1)))\n",
    "            late_fused = torch.tanh(model.late_fusion(torch.cat([early_fused, mid_fused], dim=-1)))\n",
    "            \n",
    "            fusion_stack = torch.stack([early_fused, mid_fused, late_fused], dim=1)\n",
    "            attended_features, _ = model.attention_pool(fusion_stack, fusion_stack, fusion_stack)\n",
    "            final_features = attended_features.mean(dim=1)\n",
    "            logits = model.classifier(final_features)\n",
    "        \n",
    "        # Visualize component timing\n",
    "        components = ['vision_encoder', 'text_encoder', 'cross_attention', 'classifier']\n",
    "        times = [self.results[comp]['execution_time'] for comp in components]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(components, times, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "        plt.ylabel('Execution Time (seconds)')\n",
    "        plt.title('Model Component Profiling')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels and percentages\n",
    "        total_time = sum(times)\n",
    "        for bar, time_val in zip(bars, times):\n",
    "            percentage = (time_val / total_time) * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                    f'{time_val:.3f}s\\n({percentage:.1f}%)', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {comp: self.results[comp] for comp in components}\n",
    "\n",
    "print(\"Performance optimization and benchmarking tools ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Research Experiments and Future Directions\n",
    "\n",
    "Cutting-edge research experiments and exploration of future directions in multi-modal AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Next Steps\n",
    "print(\"🎉 Advanced Multi-Modal AI Research Notebook Complete!\")\n",
    "print(\"\\n📊 What we've covered:\")\n",
    "print(\"✅ Advanced multi-modal architecture with cross-attention\")\n",
    "print(\"✅ Cutting-edge training techniques (contrastive learning, curriculum learning)\")\n",
    "print(\"✅ Comprehensive data analysis and visualization\")\n",
    "print(\"✅ Model interpretability and explainability\")\n",
    "print(\"✅ Performance optimization and benchmarking\")\n",
    "\n",
    "print(\"\\n🔬 Research Directions:\")\n",
    "print(\"• Multi-modal foundation models\")\n",
    "print(\"• Few-shot and zero-shot learning\")\n",
    "print(\"• Efficient attention mechanisms\")\n",
    "print(\"• Cross-modal knowledge distillation\")\n",
    "print(\"• Federated multi-modal learning\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"1. Implement custom datasets for your specific use case\")\n",
    "print(\"2. Experiment with different fusion strategies\")\n",
    "print(\"3. Apply advanced optimization techniques\")\n",
    "print(\"4. Conduct thorough evaluation and analysis\")\n",
    "print(\"5. Deploy optimized models to production\")\n",
    "\n",
    "print(\"\\n💡 Happy researching with Multi-Modal AI!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}